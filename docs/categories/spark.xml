<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Software Lobotomy (Posts about spark)</title><link>https://parisni.github.io/weblog</link><description></description><atom:link href="https://parisni.github.io/weblog/categories/spark.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2018 &lt;a href="mailto:nicolas.paris@riseup.net"&gt;Parisni&lt;/a&gt; </copyright><lastBuildDate>Sun, 11 Nov 2018 16:40:14 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Hbase Reflexions</title><link>https://parisni.github.io/weblog/posts/hbase-reflexion/</link><dc:creator>Nicolas Paris</dc:creator><description>&lt;div&gt;&lt;p&gt;hbase looks a powerfull tool in complement of hive. While hive suits well
for ETL and data historisation, it cannot offer sub-second access to thousand
of concurrent users.&lt;/p&gt;
&lt;p&gt;Hbase comes with a powerful compagnion aka Phoenix that provides many features,
while keeping the hbase features intact. Phoenix provides a jdbc driver to
hbase. This distinction adds many improvements:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;it offers sql, joins, secondary indexes, transactions, sequences on top of hbase.&lt;/li&gt;
&lt;li&gt;it simplifies hive, spark, solr, and general programming access to hbase
data.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="section" id="jdbc-overview"&gt;
&lt;h2&gt;JDBC overview&lt;/h2&gt;
&lt;p&gt;jdbc access can be done with &lt;a class="reference external" href="https://community.hortonworks.com/questions/47138/phoenix-query-server-connection-url-example.html"&gt;kerberos&lt;/a&gt; on the form&lt;/p&gt;
&lt;pre class="code java"&gt;&lt;a name="rest_code_e59c233bceea498da49d6380941960f2-1"&gt;&lt;/a&gt;&lt;span class="s"&gt;"jdbc:phoenix:thin:url=&amp;lt;scheme&amp;gt;://&amp;lt;server-hostname&amp;gt;:&amp;lt;port&amp;gt;;authentication=SPNEGO;principal=my_user;keytab=/home/my_user/my_user.keytab"&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Phoenix connection thought JDBC needs :
- a jdbc jar file
- the hbase-site.conf
- a kerberos ticket&lt;/p&gt;
&lt;p&gt;Phoenix does not provide a way for connection pooling. The costly part of the
connection is the hbase link. However it is cached within the region servers
and a new phoenix jdbc connection can be rebuild for every query since it is a
lightweight object.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="hive-integration"&gt;
&lt;h2&gt;Hive integration&lt;/h2&gt;
&lt;p&gt;Phoenix tables can be mounted into hive thanks to a &lt;a class="reference external" href="https://phoenix.apache.org/hive_storage_handler.html"&gt;recent plugin&lt;/a&gt;. In
comparaison to hbase plugin, this allows fast join to hive table (to be
tested). While this plugin needs phoenix 4.8.0+ HDP ships with phoenix 4.7.0.
However HDP Phoenix is a fork of Phoenix, and it integrates this feature.&lt;/p&gt;
&lt;p&gt;Because phoenix allows to mount a hbase table, hive a phoenix or a hbase table,
there is many ways to make hive querying hbase. Moreover hive can create a
regular table stored as a hbase or phoenix table wich is slightly different.
The best way is to mount phoenix as a hive external table. First advantage is
&lt;em&gt;phoenix&lt;/em&gt; allows salted tables by mean there is no need to manually handle the
key to be well distributed. Second, is loading directly into &lt;em&gt;phoenix&lt;/em&gt; allows
it to manage its secondary indexes and also some other tools such &lt;em&gt;solr&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Finally, hive can access an external &lt;em&gt;phoenix&lt;/em&gt; table and allow both upsert and
select. The plugin also provides a way to delete/update from hive, however I
haven't been able to reproduce yet since transactional tables are only
compatible with orc based tables.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="spark-integration"&gt;
&lt;h2&gt;Spark integration&lt;/h2&gt;
&lt;p&gt;Spark can read and write from phoenix &lt;a class="reference external" href="https://stackoverflow.com/questions/40329968/apache-spark-ways-to-read-and-write-from-apache-phoenix-in-java"&gt;here on SOF&lt;/a&gt; or &lt;a class="reference external" href="https://phoenix.apache.org/phoenix_spark.html"&gt;here on official&lt;/a&gt;. The good point is it can be done from scala or python.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="solr-integration"&gt;
&lt;h2&gt;Solr integration&lt;/h2&gt;
&lt;p&gt;Solr integration with hbase is a good idea: hbase contains the truth and solr allows fast lookup to discover it. However, setting up solr on top of hbase is a pain. Phoenix simplifies drastically that :
&lt;a class="reference external" href="https://nicholasmaillard.wordpress.com/2014/12/27/phoenix-to-solr-in-20-minutes/"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>hive</category><category>phoenix</category><category>solr</category><category>spark</category><guid>https://parisni.github.io/weblog/posts/hbase-reflexion/</guid><pubDate>Sat, 12 May 2018 22:00:00 GMT</pubDate></item><item><title>Spark Reflexions</title><link>https://parisni.github.io/weblog/posts/spark-reflexions/</link><dc:creator>Nicolas Paris</dc:creator><description>&lt;div&gt;&lt;p&gt;Spark is a Swiss Army Knife of a hadoop stack. It makes glue between tools and is a de-facto interface for them.&lt;/p&gt;
&lt;p&gt;Spark has a lot of advantages:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;it's a wonderfull connector: if you look at a way to connect to a tool, spark
already has a connector for you. Special mention to csv: spark is also able
to read multilined csv with quote, empty lines and special character in it.&lt;/li&gt;
&lt;li&gt;it can manipulate very huge dataset: in case the dataset does not feat in
memory, the spark partitionning feature allows spliting the tasks in working
pipelines&lt;/li&gt;
&lt;li&gt;its parallel feature can take advantage of a hadoop cluster, but is a great
solution for a laptop too&lt;/li&gt;
&lt;li&gt;the scala spark-shell is really handy for development. It's auto-completion
features are superior to any shell I've dealt with before&lt;/li&gt;
&lt;li&gt;it has SQL syntax, SPARKQL compliance, and also machine learning distributed
implementation.&lt;/li&gt;
&lt;li&gt;it can be programmed in scala, python, R, java and julia.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Spark as some disadvantages:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;time of writing, spark does not support hive ACID tables. The workaround is
to do a major compaction of the table before reading it with spark. This
means it cannot yet be integrated into a hive ETL process based on ACID
tables.&lt;/li&gt;
&lt;li&gt;spark programming needs a lot of skills. It is then not a good choice for
intermediate enterprise in comparison to hive programming. Again, hive ETL
looks a better fit for a maintainability reason.&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>spark</category><guid>https://parisni.github.io/weblog/posts/spark-reflexions/</guid><pubDate>Sat, 12 May 2018 22:00:00 GMT</pubDate></item></channel></rss>