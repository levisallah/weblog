<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Software Lobotomy (Posts about python apache-spark)</title><link>https://parisni.github.io/weblog/</link><description></description><atom:link href="https://parisni.github.io/weblog/categories/python-apache-spark.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2018 &lt;a href="mailto:nicolas.paris@riseup.net"&gt;Parisni&lt;/a&gt; </copyright><lastBuildDate>Thu, 22 Nov 2018 22:33:00 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>PySpark Considerations</title><link>https://parisni.github.io/weblog/posts/pyspark-considerations/</link><dc:creator>Parisni</dc:creator><description>&lt;div class="section" id="extracting-raw-data-from-hdfs"&gt;
&lt;h2&gt;Extracting raw data from hdfs&lt;/h2&gt;
&lt;p&gt;I have tested multiple ways to get data from hdfs. There is two situations:&lt;/p&gt;
&lt;div class="section" id="the-data-fits-in-a-local-node-ram-memory"&gt;
&lt;h3&gt;The data fits in a local node RAM memory:&lt;/h3&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: WARNING/2 (&lt;tt class="docutils"&gt;&amp;lt;string&amp;gt;&lt;/tt&gt;, line 9)&lt;/p&gt;
&lt;p&gt;Title underline too short.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
The data fits in a local node RAM memory:
========================================
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_6c8cfbd18e1e44abbb8fd070f779625a-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# will produce a local csv&lt;/span&gt;
&lt;a name="rest_code_6c8cfbd18e1e44abbb8fd070f779625a-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"select * from my_table where true"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_6c8cfbd18e1e44abbb8fd070f779625a-3"&gt;&lt;/a&gt;    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toPandas&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_6c8cfbd18e1e44abbb8fd070f779625a-4"&gt;&lt;/a&gt;    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"myLocalFile.csv"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"utf8"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_6c8cfbd18e1e44abbb8fd070f779625a-5"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# otherwise spark shell keeps running&lt;/span&gt;
&lt;a name="rest_code_6c8cfbd18e1e44abbb8fd070f779625a-6"&gt;&lt;/a&gt;&lt;span class="nb"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code bash"&gt;&lt;a name="rest_code_7a5804532f8f4db1b948e5a2cb33110b-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# run the python script:&lt;/span&gt;
&lt;a name="rest_code_7a5804532f8f4db1b948e5a2cb33110b-2"&gt;&lt;/a&gt;&lt;span class="nv"&gt;PYTHONPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my/python/path/prog.py pyspark --master yarn &lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-dataset-does-not-fit-into-ram-memory"&gt;
&lt;h3&gt;The dataset does not fit into RAM memory:&lt;/h3&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_5e9a4fa3fa4b4e7da19ff786ef721433-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# will produce some csv on hdfs&lt;/span&gt;
&lt;a name="rest_code_5e9a4fa3fa4b4e7da19ff786ef721433-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"select * from my_table where true"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_5e9a4fa3fa4b4e7da19ff786ef721433-3"&gt;&lt;/a&gt;    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;
&lt;a name="rest_code_5e9a4fa3fa4b4e7da19ff786ef721433-4"&gt;&lt;/a&gt;    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"csv"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_5e9a4fa3fa4b4e7da19ff786ef721433-5"&gt;&lt;/a&gt;    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"/output/dir/on/hdfs/"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_5e9a4fa3fa4b4e7da19ff786ef721433-6"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# otherwise spark shell keeps running&lt;/span&gt;
&lt;a name="rest_code_5e9a4fa3fa4b4e7da19ff786ef721433-7"&gt;&lt;/a&gt;&lt;span class="nb"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code bash"&gt;&lt;a name="rest_code_9fe59337cee0431fb7df493be2ceff5c-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# run the python script:&lt;/span&gt;
&lt;a name="rest_code_9fe59337cee0431fb7df493be2ceff5c-2"&gt;&lt;/a&gt;&lt;span class="nv"&gt;PYTHONPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my/python/path/prog.py pyspark --master yarn &lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
&lt;a name="rest_code_9fe59337cee0431fb7df493be2ceff5c-3"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# merge the files on the local filesystem&lt;/span&gt;
&lt;a name="rest_code_9fe59337cee0431fb7df493be2ceff5c-4"&gt;&lt;/a&gt;hadoop fs -getmerge /output/dir/on/hdfs/ /desired/local/output/file.csv
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;</description><category>python apache-spark</category><guid>https://parisni.github.io/weblog/posts/pyspark-considerations/</guid><pubDate>Thu, 22 Nov 2018 22:05:23 GMT</pubDate></item></channel></rss>